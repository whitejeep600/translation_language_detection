\documentclass[twocolumn]{article}

\usepackage{amssymb,mathrsfs,amsmath,amscd,amsthm}
\usepackage[mathcal]{euscript}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{graphicx}
\graphicspath{ {./} }

\usepackage{caption}
\usepackage{subcaption}

\usepackage[colorlinks = true,
            linkcolor=blue,
            citecolor=blue
            urlcolor=blue]{hyperref}

\hypersetup{
     urlcolor=blue,
     linkcolor=blue,
    citecolor=blue
}


\title{{\Huge \textbf{Detecting the source language of text translated by state-of-the-art Machine Translation models}} \\ }
\author{ \\ Chih-Hsiang Hsu, Chung-Hao Liao, Antoni Maciąg, Jen-Tse Wei \\ National Taiwan University}
\date{}

\newcommand{\eps}{\varepsilon}
\newcommand{\nr}[1]{\smallcaps{NR#1}}

\newcommand{\bin}{\textrm{bin}}

\newcommand{\rev}{{\mathsf R}}
\newcommand{\N}{\mathbb N}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\setof}[2]{\{#1\mid #2\}}
\newcommand{\from}{\colon}

\renewcommand{\subset}{\subseteq}
\newcommand{\aut}[1]{\mathcal {#1}}
\newcommand{\reg}[1]{\mathcal {#1}}
\newcommand{\gram}[1]{\mathcal {#1}}
\newcommand{\lang}{L}

\newcommand{\tran}[1]{\xrightarrow{#1}}

\newcommand{\produce}{\rightarrow}
\newcommand{\sep}{\mathop{\big|}}


\newcommand{\prefix}{\mathit{prefix}}
\newcommand{\infix}{\mathit{infix}}
\newcommand{\suffix}{\mathit{suffix}}
\newcommand{\pleft}{\mathit{left}}
\newcommand{\pright}{\mathit{right}}

\newcommand{\tranp}[3]{\xrightarrow{\textbf{pop}(#1), #2 ,\textbf{push}(#3)}}
\newcommand{\trant}[5]{#1,\textbf{read}(#2):\textbf{write}(#4),\textbf{state}(#3),\textbf{move}(#5)}


\begin{document}

\maketitle

\section*{Abstract}

\section*{Source language detection}

Although there is some preexisting work on tasks similar to the one we chose for the assignment, it is not a standard NLP task and to the best of our knowledge, there is no work with exactly the same problem formulation. The formulation is: given a text machine-translated into English from a known set of source languages, detect the source language. We will refer to the problem as Source Language Detection (SLD).

We are motivated by the following: in human translation, clues as to the original language in the form of both syntactic and semantic information tend to get unconsciously carried over to the translated text \cite{literary}, making it possible to detect the original language. We are curious if that is also the case for currect state-of-the-art models for Machine Translation, and if so, what kinds of models will make SLD possible and what features of the translated text will be salient for detection. If the translation models are good enough, our obtained accuracy should not be considerably higher than random guessing.

In addition, we are curious if there is a difference between translation models which were trained multilingually and ones that were not. We hypothesize that for the former, the task might be more difficult because such models have learned on languages with various syntactic structures, which could make them less likely to carry the syntactic features of a particular source language over into the translation.


\section*{Related work}

The closest work to what we attempted to do was done by Nguyen-Son et al. \cite{roundtrip} who detect, for a given English text, whether it was translated or orignally written in English, and if translated, the correct one out of a set of possible source language - translator tuples. The possible languages are Russian, German and Japanese. They use the round-translation method, utilizing the phenomenon by which, while repeatedly translating a text back and forth between two languages, each round-trip changes the text less than the previous one. Thus, given an English text $T$ which we know was translated from either Russian or German, if we generate round-trip $En \rightarrow Ru \rightarrow En$ and $En \rightarrow Ge \rightarrow En$ translations of the text, the similarity to $T$ will be higher for the translation through the language that was the original language of $T$. Therefore, the authors generate round-trip translations of a given text through a number of languages and translators, and choose the translation with the highest similarity to $T$. Its associated language-translator tuple has its own subclassifier, which is further used to determine if the text was translated or originally English. The authors prove the ability of such a model to generalize (i.e. still detect the source language) to texts translated by translators not included in training. However, a shortcoming of such an approach is that it is computationally expensive, both while training (due to training a separate subclassifier for each language-translator pair), and during inference (due to generating multiple round-trip translations).

Kurokawa et al. \cite{canada} create a model based on Support Vector Machines (SVM), capable of determining whether a text was originally English, or translated from French. They find that certain n-grams were more frequent in translated text (semantic information), but syntactic features turn out ot be powerful as well, for example there is a "higher presence of the definite article \emph{the} and prepositions in text translated from French", and "good classification accuracy was obtained even when texts were reduced to part-of-speech sequences".

Lynch \& Vogel \cite{literary}, similarly, construct an SVM based on document-level features such as number of nouns, average sentence length, word unigrams, part-of-speech (POS) bigrams. Proper names are excluded from word unigrams, as "any character or place-names could unambiguously distinguish a text". Again, the word \emph{towards} turns out to be particularly common in translation from German, and \emph{that's} (rather than \emph{that is}) - in translations from Russian. The document-level features suggest it might be easier to detect the source language of a text if it is longer, which makes such features more reliable (reduces their variance).

\section*{Approach}

\subsection*{Chosen languages}

As mentioned before, the SLD task involves detecting the source language from a given set of possible source languages. We chose Arabic, Chinese, Indonesian and Japanese to be our set. All of those are non-Indo-European languages with grammar fairly dissimilar to English, so we hypothesize that their syntactic features are carried over during translation to a bigger extent, making classification easier. For example, we hope that the rigid word order of Japanese and Chinese, which are known to be highly configurational languages, will manifest itself in the translations.

\subsection*{Dataset creation}

The creation of the dataset can be divided into two stages: gathering text in the original languages, and machine-translating it into English.

In the first stage, we scraped Wikipedia articles in each language. We wrote a script which operated as follows: a pool of articles was maintained, initialized with a single article. Until a given number of text chunks was accumulated, the script selected a random, previously unvisited article from the pool. The contents of the article, regardless of their original division into sections, paragraphs etc. were concatenated into a single piece of text. From this piece, if it was sufficiently long, one or two chunks were selected, each composed of a sequence of whole sentences. Each chunk's length (in words) was 256 or more, but would have been less than 256 without the last sentence. Then a set number of links from the article, if available, was selected at random and added to the pool.

The random link sampling, and limitation of at most two chunks from each article, were included so as to avoid the situation in which the data for a given language includes particularly many proper names related to a given topic. In such case, a model could learn to "recognize a given language" by, in fact, learning that the language is associated with that topic. These measures resulted in a very diverse dataset, with - for example - the first few chunks in the Indonesian data featuring retina cells, millenials, a Christian holiday, an Italian company, and squirrels. There was also an overlap between languages, for example Arabic data featured Christians and squirrels as well.

We decided not to remove proper names due to these measures and the fact that directly removing them from the text (or, for example, replacing them with random nouns) would have resulted in incoherent text, especially after translation, and could have ruined syntactic information.

It should be noted that we made a mistake in the scraping script, by which some of the scraped paragraphs were duplicates. We suppose this was caused by Wikipedia redirections, which make previously unseen links direct the script to articles which in fact have been seen and scraped before. This was not a significant problem, for example about 2\% of the Indonesian paragraphs were duplicates. These were removed before training.

Once the data in original languages was gathered, it had to be translated into English. We decided to generate, for each language, about 6400 chunks for the train set and further 700 for the test set. To avoid the situation in which a model learns the style of a particular translator, instead of the source language, we used a few different translators for each language.

Parts of the test set were translated using the \href{https://huggingface.co/facebook/mbart-large-50-many-to-one-mmt}{Facebook large multilingual BART} translator, since, as mentioned before, we were curious if there would be a difference in accuracy between text translated by a multilingual translator and not. In addition, since BART uses the same decoder for all languages, we suppose the translations it generates might be similar in style regardless of their source language, making it harder for the model to learn to recognize text in a given language by recognizing the styles of its associated translators.

The final dataset composition for each language (everone should describe their own, if possible):

\begin{itemize}
	\item Arabic:
	\item Chinese:
	\item Indonesian:
	\begin{itemize}
		\item Train set: 1600 chunks translated by Google Translate, the \href{https://huggingface.co/Helsinki-NLP/opus-mt-id-en}{Helsinki-NLP/opus-mt-id-en} translator, and the \href{https://huggingface.co/facebook/mbart-large-50-many-to-one-mmt}{Facebook large multilingual BART} translator each; 252 by the \href{https://www.deepl.com/en/translator}{DeepL} translator; 995  by the Microsoft translator API; and 330 by \href{https://libretranslate.com/}{LibreTranslate}; making 6377 chunks in total,
		\item Test set: 350 chunks translated by the same Helsinki and MBART translators each - 700 chunks in total.
	\end{itemize}
	These numbers were gathered before removing duplicate paragraphs, so the final numbers are, as mentioned, slightly different. In addition, due to maximum length limitations in the Helsinki translator, sentences with length (after tokenization) greater than 256 were removed.

	\item Japanese:
\end{itemize}

\subsection*{Models}

Four models have been created:

\begin{itemize}
	\item A model based on a pretrained BERT, as a straightforward but possibly powerful solution and a reference point for the others,
	\item A BERT on POS sequences, inspired by the effectiveness of POS sequences as mentioned in \cite{canada},
	\item An SVM, inspired in turn by \cite{literary},
	\item A neural network based on dependency tree convolutions and, again, POS tags.
\end{itemize}

more precise descriptions (everyone should describe their own):

\subsubsection*{Bert}

\subsubsection*{RoBERTA on POS tags}

\subsubsection*{SVM}

\subsubsection*{Dependency tree CNN}

Due to the effectiveness of source language detection based solely on syntactic information as encoded by POS sequences, we hypothesize that dependency trees, which are a rich source of such information and can encode long-range syntactic relationships missed by sequential processing, may prove powerful for SLD.

If a Deep Learning model is to be used, first it needs to be decided how to feed it the information about the dependency tree, i.e. how to represent it with a vector. Ma et al. \cite{ancestors} propose applying a convolution to the tree, as follows: first, vector representations of each word's $k$ ancestors are concatenated to its own representation in the order of ancestry (with $k$ being a hyperparameter). The vectors are padded if there are not enough ancestors. Thus we obtain a sentence representation in $l$ vectors, with $l$ being the length of the sentence. Then, $n$ (which is also a hyperparameter) learnable convolution filters are applied to each vector, with each filter applying the dot product between its own learnable vector and the input, adding learnable bias, and applying a non-linear activation. Thus we obtain $l$ features for each filter. Max-pooling is applied, i.e. the maximum feature is chosen for each filter. This way, we finally obtain an input representation with $n$ features, which can then be fed to a neural network. In addition, the authors test the effect of including siblings in the convolution, but their results show this only results in a small increase in accuracy.

It might be worth noting that a similar approach, i.e. basing the encoding of syntactic information on node-to-root paths in dependency trees, is applied by a researcher from our University, in \cite{kgan}.

Zhang et al. \cite{adjacency} propose an alternative method, by which word representations are fed straight into the network, but the network is not fully-connected and its existing connections are used to represent the graph structure. A connection exists between neuron $i$ in a layer and neuron $j$ in the next layer only if $i=j$ or the $i$-th and $j$-th words are connected in the dependency tree. Additionally, the activations each neuron obtains from the neurons in the previous layer are normalized by the in-degree of that neuron.

One may notice that neither of these approaches encodes information about the nature of dependencies between words, only the structure of the dependency tree. In fact, Zhang et al. mention that including such information does not lead to any improvement in performance.



and explain who chose one-hot POS embedding and not to include siblings oh and why dependency parsing rather than abstract meaning representation (why? syntax)
explain how sentence length, number of ancestors etc were chosen
mention that pos kinda also encodes relationship nature

\section*{Results}

introduce the test results, draw some conclusions
for every language-translator pair, number of correctly/incorrectly classified paragraphs, if possible. also ofc everyone should report their own

\section*{Conclusion}

sum up, propose further work, acknowledge shortcomings

If it turns out our model is trash, it might be either because 1. It really is trash 2. current state-of-the-art translation models are just so good

maybe we should try it on some old translation model, worse than current SOTA

compare models reduced to POS and not to validate that the whole thing was not ruined by Christians (I mean there's still like 6000 mentions of "christian" in the arabic dataset xd)

for validation we maybe should have used different translators, so that we're absolutely sure our model learned what text translated from Korean looks like, and didn't just learn what text translated by Google Translate looks like.



\section*{Work distribution}

I really hope I didn't make any mistakes in your names XD

\begin{itemize}
	\item Chih-Hsiang Hsu
	\begin{itemize}
		\item todo
		\item todo
	\end{itemize}
	
	\item Chung-Hao Liao
	\begin{itemize}
		\item todo
	\end{itemize}
	
	\item Antoni Maciąg
	\begin{itemize}
		\item todo
	\end{itemize}
	
	\item Jen-Tse Wei
	\begin{itemize}
		\item todo
	\end{itemize}
	
	\item Each member:
	\begin{itemize}
		\item Writing the part of the report about their respective dataset part and model.
	\end{itemize}
\end{itemize}


\bibliographystyle{ieeetr}
\begin{thebibliography}{99}

\bibitem{literary}
   \href{https://aclanthology.org/C12-2076.pdf}{Gerard Lynch and Carl Vogel. Towards the Automatic Detection of the Source Language of a Literary Translation. Proceedings of COLING 2012}

\bibitem{roundtrip}
   \href{https://aclanthology.org/2021.naacl-main.462.pdf }{Hoang-Quoc Nguyen-Son, Tran Phuong Thao, Seira Hidano, Ishita Gupta, and Shinsaku Kiyomoto. Machine Translated Text Detection Through Text Similarity with Round-Trip Translation Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics, 2021}

\bibitem{back}
   \href{https://arxiv.org/pdf/1910.06558.pdf}{Hoang-Quoc Nguyen-Son, Tran Phuong Thao, Seira Hidano, and Shinsaku Kiyomoto. Detecting Machine-Translated Text using Back Translation. Proceedings of the 12th International Conference on Natural Language Generation, 2019}

\bibitem{canada}
   \href{https://www.cs.cmu.edu/~dkurokaw/publications/MTS-2009-Kurokawa.pdf}{David Kurokawa, Cyril Goutte and Pierre Isabelle. Automatic Detection of Translated Text and its Impact on Machine Translation. In Proceedings of Machine Translation Summit XII, 2012}

\bibitem{ancestors}
   \href{https://aclanthology.org/P15-2029.pdf}{Mingbo Ma, Liang Huang, Bowen Zhou, Bing Xiang. Dependency-based Convolutional Neural Networks for Sentence Embedding. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, 2015}

\bibitem{adjacency}
   \href{https://aclanthology.org/D18-1244/}{Yuhao Zhang, Peng Qi, Christopher D. Manning. Graph Convolution over Pruned Dependency Trees Improves Relation Extraction. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018}

\bibitem{kgan}
   \href{https://arxiv.org/pdf/1609.03286.pdf
}{Yun-Nung Chen, Dilek Hakkani-Tur, Gokan Tur, Asli Celikyilmaz, Jianfeng Gao, and Li Deng. Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks, 2016}

\end{document}
